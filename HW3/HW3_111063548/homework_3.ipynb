{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0MrEVoVmhOy"
      },
      "source": [
        "# Computer Vision Homework 3: Big vs Small Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0hnrUlYrGWS"
      },
      "source": [
        "## Brief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_zuWZp5rSyN"
      },
      "source": [
        "Due date: Nov 13, 2023\n",
        "\n",
        "Required files: `homework-3.ipynb`, `report.pdf`\n",
        "\n",
        "To download the jupyter notebook from colab, you can refer to the colab tutorial we gave.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7423NauKQ6"
      },
      "source": [
        "## Codes for Problem 1 and Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6pBqvV6RCq"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73wanLwflUdb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "from torchvision import transforms, models, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtXEq_Yx5j-L"
      },
      "source": [
        "### Check GPU Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz3wOsYwmEz8",
        "outputId": "858a9da7-2235-4b15-8ee8-d0dc92397f38"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpaGDdwnX9g",
        "outputId": "63ca44d1-c114-4052-f65c-5a69e4e6dd45"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAoPtdOR5ojk"
      },
      "source": [
        "### Set the Seed to Reproduce the Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wphy638XBNj-"
      },
      "outputs": [],
      "source": [
        "def set_all_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_all_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmcH3NAH4wq"
      },
      "source": [
        "### Create Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VHp_O3_JgZE",
        "outputId": "e6b59874-b311-4568-d986-0b9e3d25de33"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2471, 0.2435, 0.2616)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "valid_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, pin_memory=True)   # shuffle:表示在每個訓練周期開始之前是否對數據進行随機重排\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, pin_memory=True)\n",
        "# len(train_dataloader) = 196\n",
        "# len(test_dataloader) = 40\n",
        "\n",
        "sixteenth_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//16)\n",
        "half_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//2)\n",
        "\n",
        "sixteenth_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sixteenth_train_sampler)\n",
        "half_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=half_train_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFDtcWRnFS9"
      },
      "source": [
        "### Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgZV0CodnFS9",
        "outputId": "c55df3b0-4796-4ce9-acd8-b0513ef46eb9"
      },
      "outputs": [],
      "source": [
        "# HINT: Remember to change the model to 'resnet50' and the weights to weights=\"IMAGENET1K_V1\" when needed.\n",
        "small_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights = \"IMAGENET1K_V1\")\n",
        "big_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights = \"IMAGENET1K_V1\")\n",
        "\n",
        "small_model.fc = torch.nn.Linear(small_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "big_model.fc = torch.nn.Linear(big_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "\n",
        "# Background: The original resnet18 is designed for ImageNet dataset to predict 1000 classes.\n",
        "# TODO: Change the output of the model to 10 class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZo50knhnFS_"
      },
      "source": [
        "### Training and Testing Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlXKJeYWnFTA"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in the code cell according to the pytorch tutorial we gave.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "small_model_optimizer = torch.optim.Adam(small_model.parameters(), lr=1e-3)\n",
        "big_model_optimizer = torch.optim.Adam(big_model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  num_batches = len(dataloader)\n",
        "  size = len(dataloader.dataset)\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for X, Y in tqdm(dataloader):   # tqdm:進度條\n",
        "    # X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, Y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    pred = pred.argmax(dim = 1, keepdim = True)\n",
        "    correct += pred.eq(Y.view_as(pred)).sum().item()\n",
        "\n",
        "  avg_epoch_loss = epoch_loss / num_batches\n",
        "  avg_acc = correct / size\n",
        "  return avg_epoch_loss, avg_acc\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  num_batches = len(dataloader)\n",
        "  size = len(dataloader.dataset)\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, Y in tqdm(dataloader):\n",
        "      # X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "      pred = model(X)\n",
        "\n",
        "      epoch_loss += loss_fn(pred, Y).item()\n",
        "      pred = pred.argmax(dim = 1, keepdim = True)\n",
        "      correct += pred.eq(Y.view_as(pred)).sum().item()\n",
        "\n",
        "  avg_epoch_loss = epoch_loss / num_batches\n",
        "  avg_acc = correct / size\n",
        "  return avg_epoch_loss, avg_acc\n",
        "\n",
        "# 跑for迴圈前須重置，避免累積accuracy，導致結果失真\n",
        "def reset(weight_select):\n",
        "  set_all_seed(123)\n",
        "  small_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights = weight_select)\n",
        "  big_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights = weight_select)\n",
        "  small_model.fc = torch.nn.Linear(small_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "  big_model.fc = torch.nn.Linear(big_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "  small_model_optimizer = torch.optim.Adam(small_model.parameters(), lr=1e-3)\n",
        "  big_model_optimizer = torch.optim.Adam(big_model.parameters(), lr=1e-3)\n",
        "  return small_model, small_model_optimizer, big_model, big_model_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sYq9RfWFVnwG",
        "outputId": "a077d380-d9b0-4466-c0aa-22d6041bd4b1"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "x_train_dataloader = 1.0\n",
        "x_half_train_dataloader = 0.5\n",
        "x_sixteenth_train_dataloader = 1/16\n",
        "data_size = np.array([x_sixteenth_train_dataloader, x_half_train_dataloader, x_train_dataloader])\n",
        "\n",
        "###############Small Model#################\n",
        "print(\"Starting Small Model Training\\n\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========sixteenth_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(sixteenth_train_dataloader, small_model, loss_fn, small_model_optimizer)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_sixteenth_train = small_model_train_acc\n",
        "    y_small_model_sixteenth_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========half_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(half_train_dataloader, small_model, loss_fn, small_model_optimizer)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_half_train = small_model_train_acc\n",
        "    y_small_model_half_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(train_dataloader, small_model, loss_fn, small_model_optimizer)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_train = small_model_train_acc\n",
        "    y_small_model_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "###############Big Model#################\n",
        "print(\"Starting Big Model Training\\n\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========sixteenth_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(sixteenth_train_dataloader, big_model, loss_fn, big_model_optimizer)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_sixteenth_train = big_model_train_acc\n",
        "    y_big_model_sixteenth_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========half_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(half_train_dataloader, big_model, loss_fn, big_model_optimizer)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_half_train = big_model_train_acc\n",
        "    y_big_model_half_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer, big_model, big_model_optimizer = reset(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(train_dataloader, big_model, loss_fn, big_model_optimizer)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_train = big_model_train_acc\n",
        "    y_big_model_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model_accuracy_train = np.array([y_small_model_sixteenth_train, y_small_model_half_train, y_small_model_train])\n",
        "big_model_accuracy_train = np.array([y_big_model_sixteenth_train, y_big_model_half_train, y_big_model_train])\n",
        "small_model_accuracy_test = np.array([y_small_model_sixteenth_test, y_small_model_half_test, y_small_model_test])\n",
        "big_model_accuracy_test = np.array([y_big_model_sixteenth_test, y_big_model_half_test, y_big_model_test])\n",
        "\n",
        "print(\"small_model_accuracy_train:\", small_model_accuracy_train)\n",
        "print(\"small_model_accuracy_test:\", small_model_accuracy_test)\n",
        "print(\"big_model_accuracy_train:\", big_model_accuracy_train)\n",
        "print(\"big_model_accuracy_test:\", big_model_accuracy_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqBGAUm6b5W"
      },
      "source": [
        "## Codes for Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5SBFMzPT6cP4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Small Model Training\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Big Model Training\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
            "Using cache found in C:\\Users\\zeus9/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========train_dataloader==========\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [19:51<00:00,  1.31it/s]\n",
            "100%|██████████| 313/313 [00:27<00:00, 11.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1:Loss = 1.7107 Acc = 0.41 Test_Loss = 1.7576 Test_Acc = 0.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [19:27<00:00,  1.34it/s]\n",
            "100%|██████████| 313/313 [00:28<00:00, 11.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2:Loss = 1.4558 Acc = 0.50 Test_Loss = 1.2797 Test_Acc = 0.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [20:11<00:00,  1.29it/s]\n",
            "100%|██████████| 313/313 [00:27<00:00, 11.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3:Loss = 1.1656 Acc = 0.59 Test_Loss = 1.3779 Test_Acc = 0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [19:38<00:00,  1.33it/s]\n",
            "100%|██████████| 313/313 [00:25<00:00, 12.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4:Loss = 0.9997 Acc = 0.66 Test_Loss = 0.9029 Test_Acc = 0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [18:59<00:00,  1.37it/s]\n",
            "100%|██████████| 313/313 [00:25<00:00, 12.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5:Loss = 0.9058 Acc = 0.69 Test_Loss = 0.7747 Test_Acc = 0.73\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'y_small_model_sixteenth_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17580\\2987121379.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m \u001b[0msmall_model_accuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_small_model_sixteenth_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_small_model_half_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_small_model_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[0mbig_model_accuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_big_model_sixteenth_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_big_model_half_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_big_model_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[0msmall_model_accuracy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_small_model_sixteenth_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_small_model_half_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_small_model_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_small_model_sixteenth_train' is not defined"
          ]
        }
      ],
      "source": [
        "# TODO: Try to achieve the best performance given all training data using whatever model and training strategy.\n",
        "# (New) (You cannot use the model that was pretrained on CIFAR10)\n",
        "\n",
        "def set_all_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_all_seed(123)\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "batch_size = 32   # hyperparameter\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2471, 0.2435, 0.2616)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "valid_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, pin_memory=True)   # shuffle:表示在每個訓練周期開始之前是否對數據進行随機重排\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "sixteenth_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//16)\n",
        "half_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//2)\n",
        "\n",
        "sixteenth_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sixteenth_train_sampler)\n",
        "half_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=half_train_sampler)\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "small_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights = \"IMAGENET1K_V1\")\n",
        "big_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights = \"IMAGENET1K_V1\")\n",
        "\n",
        "small_model.fc = torch.nn.Linear(small_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "big_model.fc = torch.nn.Linear(big_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "\n",
        "# 添加Dropout\n",
        "small_model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(small_model.fc.in_features, 10))\n",
        "big_model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(big_model.fc.in_features, 10))\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  num_batches = len(dataloader)\n",
        "  size = len(dataloader.dataset)\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for X, Y in tqdm(dataloader):   # tqdm:進度條\n",
        "    # X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, Y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    pred = pred.argmax(dim = 1, keepdim = True)\n",
        "    correct += pred.eq(Y.view_as(pred)).sum().item()\n",
        "\n",
        "  avg_epoch_loss = epoch_loss / num_batches\n",
        "  avg_acc = correct / size\n",
        "  return avg_epoch_loss, avg_acc\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  num_batches = len(dataloader)\n",
        "  size = len(dataloader.dataset)\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, Y in tqdm(dataloader):\n",
        "      # X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "      pred = model(X)\n",
        "\n",
        "      epoch_loss += loss_fn(pred, Y).item()\n",
        "      pred = pred.argmax(dim = 1, keepdim = True)\n",
        "      correct += pred.eq(Y.view_as(pred)).sum().item()\n",
        "\n",
        "  avg_epoch_loss = epoch_loss / num_batches\n",
        "  avg_acc = correct / size\n",
        "  return avg_epoch_loss, avg_acc\n",
        "\n",
        "def optimization(weight_select):\n",
        "    set_all_seed(123)\n",
        "    small_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights = weight_select)\n",
        "    big_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights = weight_select)\n",
        "    small_model.fc = torch.nn.Linear(small_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "    big_model.fc = torch.nn.Linear(big_model.fc.in_features, 10)  # model.fc.in_features = 512，替換新的全連接層，從1000個classes變成10個\n",
        "    loss_fn = nn.CrossEntropyLoss()   # hyperparameter\n",
        "    learning_rate = 1e-3   # hyperparameter\n",
        "    small_model_optimizer_ADAM = torch.optim.Adam(small_model.parameters(), lr=learning_rate)   # hyperparameter\n",
        "    big_model_optimizer_ADAM = torch.optim.Adam(big_model.parameters(), lr=learning_rate)   # hyperparameter\n",
        "    small_model_optimizer_SGD = torch.optim.SGD(small_model.parameters(), lr=learning_rate)   # hyperparameter\n",
        "    big_model_optimizer_SGD = torch.optim.SGD(big_model.parameters(), lr=learning_rate)   # hyperparameter\n",
        "    return small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "epochs = 5\n",
        "x_train_dataloader = 1.0\n",
        "x_half_train_dataloader = 0.5\n",
        "x_sixteenth_train_dataloader = 1/16\n",
        "data_size = np.array([x_sixteenth_train_dataloader, x_half_train_dataloader, x_train_dataloader])\n",
        "\n",
        "\n",
        "###############Small Model#################\n",
        "print(\"Starting Small Model Training\\n\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========sixteenth_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(sixteenth_train_dataloader, small_model, loss_fn, small_model_optimizer_ADAM)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_sixteenth_train = small_model_train_acc\n",
        "    y_small_model_sixteenth_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========half_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(half_train_dataloader, small_model, loss_fn, small_model_optimizer_ADAM)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_half_train = small_model_train_acc\n",
        "    y_small_model_half_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  small_model_train_loss, small_model_train_acc = train(train_dataloader, small_model, loss_fn, small_model_optimizer_ADAM)\n",
        "  small_model_test_loss, small_model_test_acc = test(valid_dataloader, small_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {small_model_train_loss:.4f} Acc = {small_model_train_acc:.2f} Test_Loss = {small_model_test_loss:.4f} Test_Acc = {small_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_small_model_train = small_model_train_acc\n",
        "    y_small_model_test = small_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "###############Big Model#################\n",
        "print(\"Starting Big Model Training\\n\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========sixteenth_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(sixteenth_train_dataloader, big_model, loss_fn, big_model_optimizer_ADAM)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_sixteenth_train = big_model_train_acc\n",
        "    y_big_model_sixteenth_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========half_train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(half_train_dataloader, big_model, loss_fn, big_model_optimizer_ADAM)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_half_train = big_model_train_acc\n",
        "    y_big_model_half_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model, small_model_optimizer_ADAM, small_model_optimizer_SGD, big_model, big_model_optimizer_ADAM, big_model_optimizer_SGD, loss_fn = optimization(\"IMAGENET1K_V1\")\n",
        "\n",
        "print(\"==========train_dataloader==========\\n\")\n",
        "for epoch in range(epochs):\n",
        "  big_model_train_loss, big_model_train_acc = train(train_dataloader, big_model, loss_fn, big_model_optimizer_ADAM)\n",
        "  big_model_test_loss, big_model_test_acc = test(valid_dataloader, big_model, loss_fn)\n",
        "  print(f\"Epoch {epoch+1:2d}:Loss = {big_model_train_loss:.4f} Acc = {big_model_train_acc:.2f} Test_Loss = {big_model_test_loss:.4f} Test_Acc = {big_model_test_acc:.2f}\")\n",
        "  if (epoch == epochs-1):\n",
        "    y_big_model_train = big_model_train_acc\n",
        "    y_big_model_test = big_model_test_acc\n",
        "print(\"Done!\")\n",
        "\n",
        "small_model_accuracy_train = np.array([y_small_model_sixteenth_train, y_small_model_half_train, y_small_model_train])\n",
        "big_model_accuracy_train = np.array([y_big_model_sixteenth_train, y_big_model_half_train, y_big_model_train])\n",
        "small_model_accuracy_test = np.array([y_small_model_sixteenth_test, y_small_model_half_test, y_small_model_test])\n",
        "big_model_accuracy_test = np.array([y_big_model_sixteenth_test, y_big_model_half_test, y_big_model_test])\n",
        "\n",
        "print(\"small_model_accuracy_train:\", small_model_accuracy_train)\n",
        "print(\"small_model_accuracy_test:\", small_model_accuracy_test)\n",
        "print(\"big_model_accuracy_train:\", big_model_accuracy_train)\n",
        "print(\"big_model_accuracy_test:\", big_model_accuracy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTSafuelpRYJ"
      },
      "source": [
        "## Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZctBdkurpQS"
      },
      "source": [
        "1. (30%) Finish the rest of the codes for Problem 1 and Problem 2 according to the hint. (2 code cells in total.)\n",
        "2. Train small model (resnet18) and big model (resnet50) from scratch on `sixteenth_train_dataloader`, `half_train_dataloader`, and `train_dataloader` respectively.\n",
        "3. (30%) Achieve the best performance given all training data using whatever model and training strategy.  \n",
        "  (You cannot use the model that was pretrained on CIFAR10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786fQTdk0msC"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsd2yTmB0k5t"
      },
      "source": [
        "Write down your insights in the report. The file name should be report.pdf.\n",
        "For the following discussion, please present the results graphically as shown in Fig. 1 and discuss them.\n",
        "\n",
        "- (30%) The relationship between the accuracy, model size, and the training dataset size.  \n",
        "    (Total 6 models. Small model trains on the sixteenth, half, and all data. Big model trains on the sixteenth, half, and all data. If the result is different from Fig.1, please explain the possible reasons.)\n",
        "- (10%) What if we train the ResNet with ImageNet initialized weights (`weights=\"IMAGENET1K_V1\"`).\n",
        "Please explain why the relationship changed this way?\n",
        "\n",
        "Hint: You can try different hyperparameters combinations when training the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWDxF-xIueMM"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXpmSj2ufkh"
      },
      "source": [
        "1. [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MemcOLK_4ULJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
